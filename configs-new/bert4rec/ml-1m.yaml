datamodule:
  dataset: "ml-1m"
  data_sources:
    num_workers: 4
    batch_size: 64
    max_seq_length: 200
    #mask_probability: 0.2
    #mask_seed: 42
    split: "leave_one_out"
    file_prefix: "ml-1m"
    train:
      type: "session"
      processors:
        - type: "cloze"
          mask_probability: 0.2
          only_last_item_mask_prob: 0.1
    validation:
      type: "session"
      processors:
        - type: "target_extractor"
        - type: "last_item_mask"
    test:
      type: "session"
      processors:
        - type: "target_extractor"
        - type: "last_item_mask"
  preprocessing:
    extraction_directory: "/home/dallmann/uni/research/datasets/ml-1m/ml-1m/preprocessed"
    output_directory: "/home/dallmann/uni/research/datasets/ml-1m/ml-1m"
templates:
  unified_output:
    path: "/tmp/ml-1m/bert4rec"
module:
  type: "bert4rec"
  metrics:
    full:
      metrics:
        mrr: [1, 5, 10]
        recall: [1, 5, 10]
        ndcg: [1, 5, 10]
    sampled:
      sample_probability_file: "ml-1m.popularity.title.txt"
      num_negative_samples: 100
      metrics:
        mrr: [ 1, 5, 10 ]
        recall: [ 1, 5, 10 ]
        ndcg: [ 1, 5, 10 ]
  model:
    max_seq_length: 200
    num_transformer_heads: 2
    num_transformer_layers: 2
    transformer_hidden_size: 64
    transformer_dropout: 0.2
features:
  item:
    column_name: "title"
    sequence_length: 200
    tokenizer:
      special_tokens:
        pad_token: "<PAD>"
        mask_token: "<MASK>"
        unk_token: "<UNK>"
      vocabulary:
        # Inferred by the datamodule
trainer:
  loggers:
    tensorboard:
    aim:
      repo: "aim://127.0.0.1:53800"
      experiment: "bert4rec_ml-1m"
  checkpoint:
    monitor: "recall@10"
    save_top_k: 3
    mode: 'max'
  early_stopping:
    min_delta: 0.001
    mode: "max"
    monitor: "recall@10"
    patience: 100
  gpus: 1
  max_epochs: 800
  check_val_every_n_epoch: 1
